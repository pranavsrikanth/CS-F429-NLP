{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b4/ca/fc1c4f8a2a4693ff437d039acf2dc93a190b9494569fbed246f535c44fc8/numpy-1.26.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m842.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m827.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/c5/2f/bf85305b044ddee0ade62c444c7ef551eb423899424b3898d60895d02f63/pandas-2.1.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.1.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/88/a3/023bf03411b8d89954d028dd56dc97f5fa04f18b4cb2acf4315dfb3dfa15/scikit_learn-1.3.1-cp39-cp39-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/77/31/b063f21370c6050a663aae5a9868d2fe112b21caeface3c248016dfea092/scipy-1.11.2-cp39-cp39-macosx_12_0_arm64.whl.metadata\n",
      "  Using cached scipy-1.11.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (54 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading numpy-1.26.0-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.1-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.1-cp39-cp39-macosx_12_0_arm64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.11.2-cp39-cp39-macosx_12_0_arm64.whl (29.6 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "Successfully installed joblib-1.3.2 numpy-1.26.0 pandas-2.1.1 pytz-2023.3.post1 scikit-learn-1.3.1 scipy-1.11.2 threadpoolctl-3.2.0 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " id                            int64\n",
      "title                        object\n",
      "body                         object\n",
      "answer_count                  int64\n",
      "comment_count                 int64\n",
      "creation_date                object\n",
      "last_activity_date           object\n",
      "last_editor_display_name     object\n",
      "owner_display_name           object\n",
      "owner_user_id               float64\n",
      "post_type_id                  int64\n",
      "score                         int64\n",
      "tags                         object\n",
      "view_count                    int64\n",
      "accepted_answer_id          float64\n",
      "favorite_count              float64\n",
      "last_edit_date               object\n",
      "last_editor_user_id         float64\n",
      "community_owned_date         object\n",
      "dtype: object\n",
      "Number of questions,columns= (20000, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read json into a dataframe\n",
    "df_idf=pd.read_json(\"data/stackoverflow-data-idf.json\",lines=True)\n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "print(\"Number of questions,columns=\",df_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradle command linei m trying to run a shell script with gradle i currently have something like this def test project tasks create test exec commandline bash c bash c my file dir script sh the problem is that i cannot run this script because i have spaces in my dir name i have tried everything e g commandline bash c bash c my file dir script sh tokenize commandline bash c bash c my file dir script sh commandline bash c new stringbuilder append bash append c my file dir script sh commandline bash c bash c my file dir script sh file dir file c my file dir script sh commandline bash c bash dir getabsolutepath im using windows bit and if i use a path without spaces the script runs perfectly therefore the only issue as i can see is how gradle handles spaces '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "\t# lowercase\n",
    "\ttext= text.lower()\n",
    "\t#remove tags <> using regex\n",
    "\ttext = re.sub(r'<.*?>', '', text) \n",
    "\t# remove special characters and digits\n",
    "\ttext= re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\treturn text\n",
    "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(pre_process) #TODO: Make appropriate call\n",
    "#show the first 'text'\n",
    "df_idf['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyasv/Downloads/Lab 4-20230923/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 131802)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "def get_stop_words(stop_file_path): \n",
    "    \"\"\"load stop words \"\"\"\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f: \n",
    "        # TODO: Read lines from the file f \n",
    "        stopwords = f.readlines()\n",
    "        # TODO: Strip all stopwords\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"resources/stopwords.txt\") #get the text column\n",
    "stopwords = list(stopwords)\n",
    "docs=df_idf['text'].tolist()\n",
    "print(len(docs))\n",
    "#create a vocabulary of words, ignore words that appear in 85% of documents, eliminate stop words\n",
    "cv = CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "# cv= CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector= cv.fit_transform(docs)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Previous definition with the parameter max_features=10000 \n",
    "cv= CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector= cv.fit_transform(docs)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serializing',\n",
       " 'private',\n",
       " 'struct',\n",
       " 'public',\n",
       " 'class',\n",
       " 'contains',\n",
       " 'properties',\n",
       " 'string',\n",
       " 'serialize',\n",
       " 'attempt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.37717703,  9.80492526,  9.51724319, ...,  8.82409601,\n",
       "       10.21039037,  9.51724319])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test docs into a dataframe and concatenate title and body\n",
    "df_test=pd.read_json(\"data/stackoverflow-test.json\",lines=True) \n",
    "df_test['text'] = df_test['title'] + df_test['body']\n",
    "df_test['text'] =df_test['text'].apply(pre_process) #TODO: Make appropriate call \n",
    "# get test docs into a list\n",
    "docs_test=df_test['text'].tolist()\n",
    "docs_title=df_test['title'].tolist()\n",
    "docs_body=df_test['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "\ttuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "\treturn sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10): \n",
    "\t\"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "\n",
    "\t#use only topn items from vector\n",
    "\tsorted_items = sorted_items[:topn+1] # TODO: Splice the data \n",
    "\tscore_vals = []\n",
    "\tfeature_vals = []\n",
    "\tfor idx, score in sorted_items:\n",
    "\t\tfname = feature_names[idx]\n",
    "\t\t#keep track of feature name and its corresponding score\n",
    "\t\t# TODO: Append score rounded off to 3 decimal places, in the score_vals list\n",
    "\t\tscore_vals.append(round(score, 3))\n",
    "\t\t# TODO: Append fname to the feature_vals list\n",
    "\t\tfeature_vals.append(feature_names[idx])\n",
    "\t#create a tuples of feature,score\n",
    "\t#results = zip(feature_vals,score_vals)\n",
    "\tresults= {}\n",
    "\tfor idx in range(len(feature_vals)):\n",
    "\t\tresults[feature_vals[idx]]=score_vals[idx] \n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Integrate War-Plugin for m2eclipse into Eclipse Project\n",
      "\n",
      "=====Body=====\n",
      "<p>I set up a small web project with JSF and Maven. Now I want to deploy on a Tomcat server. Is there a possibility to automate that like a button in Eclipse that automatically deploys the project to Tomcat?</p>\n",
      "\n",
      "<p>I read about a the <a href=\"http://maven.apache.org/plugins/maven-war-plugin/\" rel=\"nofollow noreferrer\">Maven War Plugin</a> but I couldn't find a tutorial how to integrate that into my process (eclipse/m2eclipse).</p>\n",
      "\n",
      "<p>Can you link me to help or try to explain it. Thanks.</p>\n",
      "\n",
      "===Keywords===\n",
      "eclipse 0.594\n",
      "war 0.317\n",
      "integrate 0.282\n",
      "maven 0.274\n",
      "tomcat 0.27\n",
      "plugin 0.215\n",
      "projecti 0.165\n",
      "project 0.16\n",
      "automate 0.158\n",
      "jsf 0.152\n",
      "possibility 0.147\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once\n",
    "# TODO: get_feature_names using initialized cv # get the document that we want to extract keywords from\n",
    "feature_names=cv.get_feature_names_out()\n",
    "doc=docs_test[0]\n",
    "#generate tf-idf for the given document using tfidf_transformer\n",
    "tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names, sorted_items) # TODO: Pass params\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(docs_title[0])\n",
    "print(\"\\n=====Body=====\")\n",
    "print(docs_body[0])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the common code into several methods\n",
    "def get_keywords(idx):\n",
    "\t#generate tf-idf for the given document using tfidf_transformer \n",
    "\ttf_idf_vector= tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
    "\t#sort the tf-idf vectors by descending order of scores\n",
    "\tsorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\t#extract only the top n; n here is 10\n",
    "\tkeywords=extract_topn_from_vector(feature_names, sorted_items) # TODO: Pass params\n",
    "\treturn keywords\n",
    "\n",
    "def print_results(idx,keywords):\n",
    "\t# now print the results\n",
    "\tprint(\"\\n=====Title=====\")\n",
    "\tprint(docs_title[idx])\n",
    "\tprint(\"\\n=====Body=====\")\n",
    "\tprint(docs_body[idx])\n",
    "\tprint(\"\\n===Keywords===\")\n",
    "\tfor k in keywords:\n",
    "\t\tprint(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "SQL Import Wizard - Error\n",
      "\n",
      "=====Body=====\n",
      "<p>I have a CSV file that I'm trying to import into SQL Management Server Studio.</p>\n",
      "\n",
      "<p>In Excel, the column giving me trouble looks like this:\n",
      "<a href=\"https://i.stack.imgur.com/pm0uS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pm0uS.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>Tasks > import data > Flat Source File > select file</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/G4b6I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/G4b6I.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>I set the data type for this column to DT_NUMERIC, adjust the DataScale to 2 in order to get 2 decimal places, but when I click over to Preview, I see that it's clearly not recognizing the numbers appropriately:</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/NZhiQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NZhiQ.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>The column mapping for this column is set to type = decimal; precision 18; scale 2.</p>\n",
      "\n",
      "<p>Error message: Data Flow Task 1: Data conversion failed. The data conversion for column \"Amount\" returned status value 2 and status text \"The value could not be converted because of a potential loss of data.\".\n",
      " (SQL Server Import and Export Wizard)</p>\n",
      "\n",
      "<p>Can someone identify where I'm going wrong here?  Thanks!</p>\n",
      "\n",
      "===Keywords===\n",
      "column 0.364\n",
      "import 0.285\n",
      "data 0.282\n",
      "wizard 0.271\n",
      "decimal 0.226\n",
      "conversion 0.223\n",
      "sql 0.217\n",
      "status 0.164\n",
      "file 0.147\n",
      "appropriately 0.142\n",
      "flat 0.138\n"
     ]
    }
   ],
   "source": [
    "idx=120 \n",
    "keywords=get_keywords(idx) \n",
    "print_results(idx,keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>serializing a private struct can it be done i ...</td>\n",
       "      <td>{'eclipse': 0.594, 'war': 0.317, 'integrate': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i prevent floated right content from ov...</td>\n",
       "      <td>{'evaluate': 0.473, 'content': 0.404, 'console...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gradle command linei m trying to run a shell s...</td>\n",
       "      <td>{'dynamic': 0.41, 'performed': 0.354, 'operati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loop variable as parameter in asynchronous fun...</td>\n",
       "      <td>{'image': 0.424, 'jpg': 0.412, 'background': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canot get the href valuehi i need to valid the...</td>\n",
       "      <td>{'uri': 0.371, 'bitmap': 0.318, 'intent': 0.30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>how to unbind click and click submit button in...</td>\n",
       "      <td>{'delphi': 0.617, 'compatible': 0.364, 'win': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>swaggerui auth redirect swaggeruiauth of nulli...</td>\n",
       "      <td>{'node': 0.548, 'selectsinglenode': 0.303, 'nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>ssrs value display error for ssrs conditional ...</td>\n",
       "      <td>{'logo': 0.553, 'step': 0.332, 'triangle': 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>accessing and changing a class instance from a...</td>\n",
       "      <td>{'length': 0.426, 'ev': 0.415, 'introduce': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>how to print the current time in the format da...</td>\n",
       "      <td>{'oauth': 0.401, 'sdk': 0.339, 'redirect': 0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   doc  \\\n",
       "0    serializing a private struct can it be done i ...   \n",
       "1    how do i prevent floated right content from ov...   \n",
       "2    gradle command linei m trying to run a shell s...   \n",
       "3    loop variable as parameter in asynchronous fun...   \n",
       "4    canot get the href valuehi i need to valid the...   \n",
       "..                                                 ...   \n",
       "495  how to unbind click and click submit button in...   \n",
       "496  swaggerui auth redirect swaggeruiauth of nulli...   \n",
       "497  ssrs value display error for ssrs conditional ...   \n",
       "498  accessing and changing a class instance from a...   \n",
       "499  how to print the current time in the format da...   \n",
       "\n",
       "                                              keywords  \n",
       "0    {'eclipse': 0.594, 'war': 0.317, 'integrate': ...  \n",
       "1    {'evaluate': 0.473, 'content': 0.404, 'console...  \n",
       "2    {'dynamic': 0.41, 'performed': 0.354, 'operati...  \n",
       "3    {'image': 0.424, 'jpg': 0.412, 'background': 0...  \n",
       "4    {'uri': 0.371, 'bitmap': 0.318, 'intent': 0.30...  \n",
       "..                                                 ...  \n",
       "495  {'delphi': 0.617, 'compatible': 0.364, 'win': ...  \n",
       "496  {'node': 0.548, 'selectsinglenode': 0.303, 'nu...  \n",
       "497  {'logo': 0.553, 'step': 0.332, 'triangle': 0.3...  \n",
       "498  {'length': 0.426, 'ev': 0.415, 'introduce': 0....  \n",
       "499  {'oauth': 0.401, 'sdk': 0.339, 'redirect': 0.3...  \n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate tf-idf for all documents in your list. docs_test has 500 documents using tfidf_transformer\n",
    "# TODO. HINT - Use docs_test for the transform() of cv\n",
    "tf_idf_vector= tfidf_transformer.transform(cv.transform(docs_test))\n",
    "results=[]\n",
    "for i in range(tf_idf_vector.shape[0]):\n",
    "\t# get vector for a single document\n",
    "\tvector = tf_idf_vector[i]\n",
    "\t#sort the tf-idf vector by descending order of scores\n",
    "\tsorted_items=sort_coo(vector.tocoo())\n",
    "\tkeywords=extract_topn_from_vector(feature_names, sorted_items)\n",
    "\tresults.append(keywords) \n",
    "df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loves': [0.2, 0.0, 0.25], 'really': [0.4, 0.0, 0.0], 'coffee': [0.2, 0.25, 0.0], 'he': [0.2, 0.0, 0.0], 'dislikes': [0.0, 0.25, 0.0], 'my': [0.0, 0.25, 0.25], 'sister': [0.0, 0.25, 0.25], 'tea': [0.0, 0.0, 0.25]}\n",
      "{'loves': 2, 'really': 1, 'coffee': 2, 'he': 1, 'dislikes': 1, 'my': 2, 'sister': 2, 'tea': 1}\n",
      "\n",
      "Document 1:\n",
      "['he', 'really', 'really', 'loves', 'coffee']\n",
      "[0.03521825181113625, 0.19084850188786498, 0.03521825181113625, 0.09542425094393249, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Document 2:\n",
      "['my', 'sister', 'dislikes', 'coffee']\n",
      "[0.0, 0.0, 0.04402281476392031, 0.0, 0.11928031367991561, 0.04402281476392031, 0.04402281476392031, 0.0]\n",
      "\n",
      "Document 3:\n",
      "['my', 'sister', 'loves', 'tea']\n",
      "[0.04402281476392031, 0.0, 0.0, 0.0, 0.0, 0.04402281476392031, 0.04402281476392031, 0.11928031367991561]\n"
     ]
    }
   ],
   "source": [
    "# DATA BLOCK\n",
    "# Each new line (after '\\n') represents a new document\n",
    "text = '''he really really loves coffee\n",
    "my sister dislikes coffee\n",
    "my sister loves tea'''\n",
    "import math\n",
    "\n",
    "def main(text):\n",
    "\t# split the text first into lines and then into lists of words \n",
    "\tdocs = [line.split() for line in text.split('\\n')]\n",
    "\tN = len(docs)\n",
    "\t# create the vocabulary: the list of words that appear at leastonce\n",
    "\tvocabulary = list(set([word for doc in docs for word in doc]))\n",
    "\t# print(vocabulary)\n",
    "\tdf = {}\n",
    "\ttf = {}\n",
    "\tfor word in vocabulary:\n",
    "\t\t# tf: number of occurrences of word w in document divided by document length\n",
    "\t\t# note: tf[word] will be a list containing the tf of each word for each document\n",
    "\t\t# for example tf['he'][0] contains the term frequence of the word 'he' in the first document\n",
    "\t\ttf[word] = [doc.count(word)/len(doc) for doc in docs]\n",
    "\t\t# df: number of documents containing word w\n",
    "\t\tdf[word] = sum([word in doc for doc in docs])\n",
    "\t# loop through documents to calculate the tf-idf values\n",
    "\tprint(tf)\n",
    "\tprint(df)\n",
    "\tfor doc_index, doc in enumerate(docs): \n",
    "\t\ttfidf = []\n",
    "\t\tprint('\\nDocument %d:' % (doc_index + 1))\n",
    "\t\tprint(doc)\n",
    "\t\tfor word in vocabulary:\n",
    "\t\t\t# print(word, end=' ')\n",
    "\t\t\tword_tf = tf[word][doc_index]\n",
    "\t\t\t# print(tf, end=' ')\n",
    "\t\t\tword_df = df[word]\n",
    "\t\t\t# print(word_df, end=' ')\n",
    "\t\t\t# TODO (Don’t forget the base for log is 10)\n",
    "\t\t\tword_tfidf = word_tf * math.log(N/word_df,10)\n",
    "\t\t\ttfidf.append(word_tfidf)\n",
    "\t\tprint(tfidf)\n",
    "main(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
